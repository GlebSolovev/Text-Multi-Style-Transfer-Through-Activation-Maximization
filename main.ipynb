{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "import datasets\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import numpy as np\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from classifiers.sttbt.sentiment_classifier import SentimentSTTBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "base_vocab = torchtext.vocab.vocab(tokenizer.vocab, min_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_vocab_to_vocab_fn(vocab_from, vocab_to):\n",
    "    len_from, len_to = len(vocab_from), len(vocab_to)\n",
    "    matrix = torch.zeros((len_to, len_from))\n",
    "\n",
    "    for i in range(len_from):\n",
    "        token = vocab_from.lookup_token(i)\n",
    "        if not vocab_to.__contains__(token):\n",
    "            matrix[0, i] = 1\n",
    "        else:\n",
    "            matrix[vocab_to[token], i] = 1\n",
    "\n",
    "    matrix = matrix.permute(1, 0).to(torch.float)\n",
    "\n",
    "    def helper(input):\n",
    "        return input @ matrix\n",
    "\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://github.com/maknotavailable/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L172-L200\n",
    "class OneHotBertEmbeddings(nn.Module):\n",
    "    def __init__(self, bert_embeddings):\n",
    "        super().__init__()\n",
    "        self.word_embeddings = bert_embeddings.word_embeddings.weight.clone()\n",
    "        self.position_embeddings = bert_embeddings.position_embeddings\n",
    "        self.layer_norm = bert_embeddings.LayerNorm\n",
    "\n",
    "    def forward(self, one_hot_encoded):\n",
    "        batch_size, seq_length, _ = one_hot_encoded.size()\n",
    "        position_ids = torch.arange(seq_length, dtype=torch.long, device=one_hot_encoded.device)\n",
    "        position_ids = position_ids.unsqueeze(0).expand((batch_size, seq_length))\n",
    "\n",
    "        words_embeddings = one_hot_encoded @ self.word_embeddings\n",
    "        position_embeddings = self.position_embeddings(position_ids)\n",
    "\n",
    "        embeddings = words_embeddings + position_embeddings\n",
    "        embeddings = self.layer_norm(embeddings)\n",
    "        return embeddings\n",
    "\n",
    "\n",
    "class OneHotBertModel(nn.Module):\n",
    "    def __init__(self, bert_model):\n",
    "        super().__init__()\n",
    "        self.embeddings = OneHotBertEmbeddings(bert_model.embeddings)\n",
    "        self.encoder = bert_model.encoder\n",
    "        self.pooler = bert_model.pooler\n",
    "\n",
    "    def forward(self, input):\n",
    "        \n",
    "        batch_size, seq_len, vocab_size = input.size()\n",
    "\n",
    "        embedding_output = self.embeddings(input)\n",
    "        encoded_layers = self.encoder(embedding_output,\n",
    "                                      torch.zeros(batch_size, 1, 1, seq_len),\n",
    "                                      output_all_encoded_layers=True)\n",
    "        sequence_output = encoded_layers[-1]\n",
    "        pooled_output = self.pooler(sequence_output)\n",
    "        encoded_layers = encoded_layers[-1]\n",
    "        return encoded_layers, pooled_output\n",
    "\n",
    "\n",
    "class OneHotInputModel(nn.Module):\n",
    "    def __init__(self, model, from_vocab, to_vocab):\n",
    "        super().__init__()\n",
    "        self.transform_matrix = from_vocab_to_vocab_fn(from_vocab, to_vocab)\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input):\n",
    "        transformed = self.transform_matrix(input)\n",
    "        output = self.model(transformed)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_args = {\"max_text_length_in_tokens\": 50, \"gpu\": False}\n",
    "sentiment_sttbt = SentimentSTTBTClassifier(batch_size=1, **additional_args)\n",
    "sentiment_model = sentiment_sttbt.model\n",
    "sentiment_vocab = torchtext.vocab.vocab(sentiment_sttbt.src_dict.labelToIdx,min_freq=0)\n",
    "sentiment_classifier = OneHotInputModel(\n",
    "    model = sentiment_model,\n",
    "    from_vocab = base_vocab,\n",
    "    to_vocab = sentiment_vocab\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_latent(length, vocab):\n",
    "    return torch.rand((length, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_latent(latent, num_samples, vocab, verbose=False):\n",
    "    probas = F.softmax(latent, dim=1)\n",
    "    sampled_indecies = torch.multinomial(probas, num_samples=num_samples, replacement=True)\n",
    "    if verbose:\n",
    "        print(sampled_indecies)\n",
    "    one_hot_encoded = F.one_hot(sampled_indecies, num_classes=len(vocab))\n",
    "    return one_hot_encoded.permute(1, 0, 2).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_sample(sampled, vocab):\n",
    "    numpy_sampled = sampled.detach().clone().numpy()\n",
    "    argmaxed = np.argmax(numpy_sampled, axis=-1)\n",
    "    return [' '.join(vocab.lookup_tokens(sample)) for sample in argmaxed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, num_samples, vocab):\n",
    "        self.sampler = lambda latent: sample_from_latent(\n",
    "            latent = latent,\n",
    "            num_samples = num_samples,\n",
    "            vocab = vocab,\n",
    "        )\n",
    "\n",
    "class EmbeddingsContentLoss(Loss):\n",
    "    def __init__(self, target_ids, embeddings, num_samples, vocab):\n",
    "        super().__init__(num_samples, vocab)\n",
    "\n",
    "        self.intitial_vectors = embeddings(target_ids).detach().clone()\n",
    "        print(self.intitial_vectors.shape)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.embeddings = embeddings.weight.clone()\n",
    "\n",
    "    def __call__(self, latent):\n",
    "        sampled = self.sampler(latent).requires_grad_(True)\n",
    "        embedded = sampled @ self.embeddings\n",
    "\n",
    "        return lambda: sampled.grad.mean(dim=0), self.criterion(embedded, self.intitial_vectors.unsqueeze(0).expand_as(embedded))\n",
    "\n",
    "\n",
    "class BertContentLoss(Loss):\n",
    "    def __init__(self, target, bert, criterion, num_samples, vocab):\n",
    "        super().__init__(num_samples, vocab)\n",
    "        self.bert = bert\n",
    "        target_output, _ = self.bert(target.unsqueeze(0).to(torch.float))\n",
    "        self.target_output = target_output.detach()\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, latent):\n",
    "        sampled = self.sampler(latent).requires_grad_(True)\n",
    "        output, _ = self.bert(sampled)\n",
    "\n",
    "        return lambda: sampled.grad.mean(dim = 0), self.criterion(output, self.target_output.expand_as(output))\n",
    "        \n",
    "\n",
    "class StyleLoss(Loss):\n",
    "    def __init__(self, classificators, target, num_samples, vocab):\n",
    "        super().__init__(num_samples, vocab)\n",
    "        self.classificators = classificators\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.target = target\n",
    "\n",
    "    def __call__(self, latent):\n",
    "        sampled = self.sampler(latent = latent).requires_grad_(True)\n",
    "        # sampled = [num_samples, len, vocab_size]\n",
    "\n",
    "\n",
    "        scores = torch.stack([predict(sampled.permute(1, 0, 2)) for predict in self.classificators])\n",
    "        # scores = [num_classificators, num_samples, ouput_dim]\n",
    "\n",
    "        target = self.target.unsqueeze(0).expand_as(scores).to(torch.float)\n",
    "\n",
    "        return lambda: sampled.grad.mean(dim = 0), self.criterion(scores, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "tensor([2023, 2143, 2003, 6659,  999,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100/4226325222.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  one_hot_target = F.one_hot(torch.tensor(ids), num_classes=len(base_vocab))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 300])\n"
     ]
    }
   ],
   "source": [
    "text = \"This film is terrible !\"\n",
    "tokens = [base_vocab[t] for t in tokenizer.tokenize(text)]\n",
    "ids = F.pad(torch.tensor(tokens), pad=(0, 50 - len(tokens)), value=0)\n",
    "print(ids.shape)\n",
    "one_hot_target = F.one_hot(torch.tensor(ids), num_classes=len(base_vocab))\n",
    "print(ids)\n",
    "embeddings = nn.Embedding(len(base_vocab), 300)\n",
    "vectors = torchtext.vocab.GloVe('6B', dim=300)\n",
    "embeddings.weight.data = vectors.get_vecs_by_tokens(base_vocab.get_itos())\n",
    "\n",
    "losses_fn = [\n",
    "    (1, StyleLoss(\n",
    "        classificators = [sentiment_classifier],\n",
    "        target = torch.tensor(1),\n",
    "        num_samples = 16,\n",
    "        vocab = base_vocab\n",
    "    )),\n",
    "    (0.1, EmbeddingsContentLoss(\n",
    "        target_ids = ids,\n",
    "        embeddings = embeddings,\n",
    "        num_samples = 16,\n",
    "        vocab = base_vocab,\n",
    "    ))\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [01:33<00:00,  4.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09569944441318512\n",
      "tensor([[ 7688, 12455,  6254, 22510, 27962],\n",
      "        [ 3798,  1058, 21698, 16723, 19193],\n",
      "        [ 8489,  3769,  4201,  3342,  5568],\n",
      "        [14992,  2517, 13946, 26165, 22438],\n",
      "        [11673, 10315, 17443,  3058, 11958],\n",
      "        [ 9508,  5696, 21459,  4784,  4058],\n",
      "        [28645, 17576,  5557, 10025, 18114],\n",
      "        [ 2777,  7023,  8642, 23302, 10972],\n",
      "        [ 5822,  8208,  5030, 10156, 19882],\n",
      "        [ 5129, 13467,  1027, 21516,  5215],\n",
      "        [ 7168,  2295, 23658,  9393,  7644],\n",
      "        [15896, 13082, 26071, 16733,  4056],\n",
      "        [13364, 23705, 21722, 22421, 11310],\n",
      "        [ 1043,  3341,  7689,  5132, 12806],\n",
      "        [27343, 14414,  3857, 11780,  9678],\n",
      "        [17530,  3225, 23893,  3530,  3913],\n",
      "        [16416, 30111, 17544,  5202, 18237],\n",
      "        [11441,  9903, 26199, 17102, 26007],\n",
      "        [ 5664,  5541, 14466, 27997,  7258],\n",
      "        [ 2272, 19723, 26380, 17901,  5492],\n",
      "        [14716, 16236, 12849, 17070,  5232],\n",
      "        [ 2680, 12528,  6167, 14957,  9657],\n",
      "        [20482,  7906,  2519,  7146,  3016],\n",
      "        [ 7166, 20749,  7250, 26734, 15339],\n",
      "        [ 6857,  7170,  6271, 28144,  4248],\n",
      "        [ 3435, 21674, 11718,  5391,  7182],\n",
      "        [11867,  8600, 19394, 10392, 23281],\n",
      "        [ 4980,  2348, 24945, 27618,  5446],\n",
      "        [ 2783, 17170,  6796,  6521,  1690],\n",
      "        [11834,  5519,  7728,  7514, 12739],\n",
      "        [26925, 26159,  5220,  6302,  3563],\n",
      "        [19997, 10733, 28242, 15220, 25005],\n",
      "        [ 2925, 24513, 19971, 19471, 29254],\n",
      "        [13055,  4832, 18669,  2920,  3454],\n",
      "        [ 7294,  2264, 10849,  6181, 15835],\n",
      "        [ 2644,  4099, 24750,  2887, 12450],\n",
      "        [ 5465,  4060, 12245,  5566, 13951],\n",
      "        [16428,  5065, 11210,  4924, 11072],\n",
      "        [26568, 10017, 29407,  5202, 22241],\n",
      "        [20094, 26598,  4007,  3769, 16607],\n",
      "        [ 5328,  7478,   197, 11543, 23893],\n",
      "        [22666, 12599, 25568,  3076,  6947],\n",
      "        [ 5855, 27503, 14927,  4527,  9693],\n",
      "        [ 2033,  6669,  2191,  3635,  1433],\n",
      "        [ 2608,  7098, 27486, 21475, 24165],\n",
      "        [27044,  4493,  9144,  8483, 18937],\n",
      "        [ 2240,  7224,  2622,  3164,  9401],\n",
      "        [26694, 23264, 24091, 13012,  7211],\n",
      "        [  394, 26812, 24577,  8558, 11035],\n",
      "        [10090,  5191, 18980,  4060,  9611]])\n",
      "['destination levels praise thee balcony rosa gaines met gardens surrounded eyed obsessed promptly g distinguishes beads ##rea diagnosed distinct come ##croft recorded smoked tend rocky fast sp antonio current chat campos stacey future grandma hitting stop kings rd terri larsen views affluent photography me arms secluded line sharpened [unused389] ruby', 'folks v pop written repairs christopher prefix confidence removal exceed though poster converse edge halloween starting ##← varieties creative reg mourning clip keeps aesthetics load toilets daddy although tee twin kade pizza ##ffey stands east enemy pick bachelor casual souza ranging labeled flawless perfectly proud existing atmosphere boon £10 worried', 'document oversized laid proving hoover splendid andy permanently wedding = installing bottled vip pairs build puff yielded ##estra polytechnic delightful ko strip feet ballet blow precision compliment regulars justin quarters familiar ##rgeon tammy lori deserved conical jess wherever figurative software [unused192] carts offerings make quieter deals project rhino compliant ##grant', '##khar accusing remember frankish date ideas downstairs ##physical glowing crimea michelle piston kettle blues borrowed agreed dragon disciples carpets weed gem dart trio buds dh bound fantastic ##uram sessions reply photo bowls ounce involved knee japanese compete moscow dragon pop differently student surprised weight steak ranges regional tri thereby pick', 'snacks nj grass raked tina ohio energetic donna bonding clark scores dedicated projected naomi luther lay peach pinnacle spaces steam walker confident bay denise quick sufficient whoa finance を paired specific mcdowell scents programs arguably quantities assisting suits cantonese instinctively puff proof motors འ sausage scotch nina beijing maple solve']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [00:18<01:42,  6.05s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=12'>13</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m coeff \u001b[39m*\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=13'>14</a>\u001b[0m     grads\u001b[39m.\u001b[39mappend(grad_fn)    \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=14'>15</a>\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=16'>17</a>\u001b[0m total_grad \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m grad_fn: grad_fn(), grads))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent = init_latent(length = 50, vocab = base_vocab)\n",
    "latent = latent.requires_grad_(True)\n",
    "optimizer = torch.optim.Adam((latent.requires_grad_(True),), lr=0.1)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    for iteration in tqdm(range(20)):\n",
    "        latent.requires_grad_(False)\n",
    "        total_loss = torch.tensor(0, dtype=torch.float)\n",
    "        grads = []\n",
    "        for coeff, loss_fn in losses_fn:\n",
    "            grad_fn, loss = loss_fn(latent)\n",
    "            total_loss += coeff * loss\n",
    "            grads.append(grad_fn)    \n",
    "        total_loss.backward()\n",
    "\n",
    "        total_grad = sum(map(lambda grad_fn: grad_fn(), grads))\n",
    "        optimizer.zero_grad()\n",
    "        latent.requires_grad_(True)\n",
    "        F.softmax(latent, dim=1).backward(gradient = total_grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        grads.append(latent.grad.norm().item())\n",
    "    \n",
    "    print(total_loss.item())\n",
    "    sampled = sample_from_latent(latent, 5, base_vocab, verbose=True)\n",
    "    print(text_from_sample(sampled, base_vocab))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
