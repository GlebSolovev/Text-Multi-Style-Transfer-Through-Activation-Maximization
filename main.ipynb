{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchtext\n",
    "from tqdm import tqdm\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
    "from classifiers.sttbt.sentiment_classifier import SentimentSTTBTClassifier\n",
    "from classifiers.xlmr.formality_classifier import FormalityXLMRClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "base_vocab = torchtext.vocab.vocab(tokenizer.vocab, min_freq=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def from_vocab_to_vocab_fn(vocab_from, vocab_to):\n",
    "    len_from, len_to = len(vocab_from), len(vocab_to)\n",
    "    matrix = torch.zeros((len_to, len_from))\n",
    "\n",
    "    for i in range(len_from):\n",
    "        token = vocab_from.lookup_token(i)\n",
    "        if not vocab_to.__contains__(token):\n",
    "            matrix[0, i] = 1\n",
    "        else:\n",
    "            matrix[vocab_to[token], i] = 1\n",
    "\n",
    "    matrix = matrix.permute(1, 0).to(torch.float)\n",
    "\n",
    "    def helper(input):\n",
    "        return input @ matrix\n",
    "\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneHotEmbeddings(nn.Module):\n",
    "    def __init__(self, lookup_embeddings):\n",
    "        super().__init__()\n",
    "        self.embeddings = lookup_embeddings.weight.clone()\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        return inputs @ self.embeddings\n",
    "\n",
    "\n",
    "class OneHotInputModel(nn.Module):\n",
    "    def __init__(self, lookup_embeddings, model, from_vocab, to_vocab):\n",
    "        super().__init__()\n",
    "        self.transform_matrix = from_vocab_to_vocab_fn(from_vocab, to_vocab)\n",
    "        self.one_hot_embeddings = OneHotEmbeddings(lookup_embeddings)\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, input):\n",
    "        transformed = self.transform_matrix(input)\n",
    "        embedded = self.one_hot_embeddings(transformed)\n",
    "        output = self.model(input_embeds = embedded)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_sttbt = SentimentSTTBTClassifier(batch_size=1, max_text_length_in_tokens=100,)\n",
    "sentiment_model = sentiment_sttbt.model\n",
    "sentiment_vocab = torchtext.vocab.vocab(sentiment_sttbt.src_dict.labelToIdx,min_freq=0)\n",
    "sentiment_classifier = OneHotInputModel(\n",
    "    lookup_embeddings = sentiment_model.word_lut,\n",
    "    model = sentiment_model,\n",
    "    from_vocab = base_vocab,\n",
    "    to_vocab = sentiment_vocab\n",
    ")\n",
    "\n",
    "# very big transformation matrix\n",
    "\n",
    "# formality_xlm = FormalityXLMRClassifier()\n",
    "# formality_vocab = torchtext.vocab.vocab(formality_xlm.tokenizer.vocab,min_freq=0)\n",
    "# formality_classifier = OneHotInputModel(\n",
    "#     lookup_embeddings = formality_xlm.model.roberta.embeddings.word_embeddings,\n",
    "#     model = formality_xlm.model,\n",
    "#     from_vocab=base_vocab,\n",
    "#     to_vocab=formality_vocab,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_latent(length, vocab):\n",
    "    return torch.rand((length, len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_latent(latent, num_samples, vocab, verbose=False):\n",
    "    probas = F.softmax(latent, dim=1)\n",
    "    sampled_indecies = torch.multinomial(probas, num_samples=num_samples, replacement=True)\n",
    "    if verbose:\n",
    "        print(sampled_indecies)\n",
    "    one_hot_encoded = F.one_hot(sampled_indecies, num_classes=len(vocab))\n",
    "    return one_hot_encoded.permute(1, 0, 2).to(torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_sample(sampled, vocab, length = None):\n",
    "    numpy_sampled = sampled[:, :(-1 if length is None else length)].detach().clone().numpy()\n",
    "    argmaxed = np.argmax(numpy_sampled, axis=-1)\n",
    "    return [' '.join(vocab.lookup_tokens(sample)) for sample in argmaxed]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss:\n",
    "    def __init__(self, num_samples, vocab):\n",
    "        self.sampler = lambda latent: sample_from_latent(\n",
    "            latent = latent,\n",
    "            num_samples = num_samples,\n",
    "            vocab = vocab,\n",
    "        )\n",
    "\n",
    "class EmbeddingsContentLoss(Loss):\n",
    "    def __init__(self, target_ids, embeddings, num_samples, vocab):\n",
    "        super().__init__(num_samples, vocab)\n",
    "\n",
    "        self.intitial_vectors = embeddings(target_ids).detach().clone()\n",
    "        print(self.intitial_vectors.shape)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.embeddings = embeddings.weight.clone()\n",
    "\n",
    "    def __call__(self, latent):\n",
    "        sampled = self.sampler(latent).requires_grad_(True)\n",
    "        embedded = sampled @ self.embeddings\n",
    "\n",
    "        return lambda: sampled.grad.mean(dim=0), self.criterion(embedded, self.intitial_vectors.unsqueeze(0).expand_as(embedded))\n",
    "\n",
    "\n",
    "class BertContentLoss(Loss):\n",
    "    def __init__(self, target, bert, criterion, num_samples, vocab):\n",
    "        super().__init__(num_samples, vocab)\n",
    "        self.bert = bert\n",
    "        target_output, _ = self.bert(target.unsqueeze(0).to(torch.float))\n",
    "        self.target_output = target_output.detach()\n",
    "        self.criterion = criterion\n",
    "\n",
    "    def __call__(self, latent):\n",
    "        sampled = self.sampler(latent).requires_grad_(True)\n",
    "        output, _ = self.bert(sampled)\n",
    "\n",
    "        return lambda: sampled.grad.mean(dim = 0), self.criterion(output, self.target_output.expand_as(output))\n",
    "        \n",
    "\n",
    "class StyleLoss(Loss):\n",
    "    def __init__(self, classificators, target, num_samples, vocab):\n",
    "        super().__init__(num_samples, vocab)\n",
    "        self.classificators = classificators\n",
    "        self.criterion = nn.BCELoss()\n",
    "        self.target = target\n",
    "\n",
    "    def __call__(self, latent):\n",
    "        sampled = self.sampler(latent = latent).requires_grad_(True)\n",
    "        # sampled = [num_samples, len, vocab_size]\n",
    "\n",
    "\n",
    "        scores = torch.stack([predict(sampled) for predict in self.classificators])\n",
    "        # scores = [num_classificators, num_samples, ouput_dim]\n",
    "\n",
    "        target = self.target.unsqueeze(0).expand_as(scores).to(torch.float)\n",
    "\n",
    "        return lambda: sampled.grad.mean(dim = 0), self.criterion(scores, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50])\n",
      "tensor([2023, 2143, 2003, 6659,  999,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_242/1239254434.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  one_hot_target = F.one_hot(torch.tensor(ids), num_classes=len(base_vocab))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 300])\n"
     ]
    }
   ],
   "source": [
    "text = \"This film is terrible !\"\n",
    "tokens = [base_vocab[t] for t in tokenizer.tokenize(text)]\n",
    "ids = F.pad(torch.tensor(tokens), pad=(0, 50 - len(tokens)), value=0)\n",
    "print(ids.shape)\n",
    "one_hot_target = F.one_hot(torch.tensor(ids), num_classes=len(base_vocab))\n",
    "print(ids)\n",
    "embeddings = nn.Embedding(len(base_vocab), 300)\n",
    "vectors = torchtext.vocab.GloVe('6B', dim=300)\n",
    "embeddings.weight.data = vectors.get_vecs_by_tokens(base_vocab.get_itos())\n",
    "\n",
    "losses_fn = [\n",
    "    (1, StyleLoss(\n",
    "        classificators = [sentiment_classifier],\n",
    "        target = torch.tensor(1),\n",
    "        num_samples = 16,\n",
    "        vocab = base_vocab\n",
    "    )),\n",
    "    (1, EmbeddingsContentLoss(\n",
    "        target_ids = ids,\n",
    "        embeddings = embeddings,\n",
    "        num_samples = 16,\n",
    "        vocab = base_vocab,\n",
    "    ))\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|‚ñç         | 2/50 [00:12<04:58,  6.23s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb Cell 11'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=13'>14</a>\u001b[0m     total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m coeff \u001b[39m*\u001b[39m loss\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=14'>15</a>\u001b[0m     grads\u001b[39m.\u001b[39mappend(grad_fn)    \n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=15'>16</a>\u001b[0m total_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=17'>18</a>\u001b[0m total_grad \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m grad_fn: grad_fn(), grads))\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/kibrq/workspace/deep-learning/text-multi-style-transfer/main.ipynb#ch0000010?line=18'>19</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    355\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    356\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    357\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    361\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    362\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 363\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "latent = init_latent(length = 50, vocab = base_vocab)\n",
    "# length >= 50 as sentiment classifier cannot accept less\n",
    "latent = latent.requires_grad_(True)\n",
    "optimizer = torch.optim.Adam((latent.requires_grad_(True),), lr=0.6)\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    for iteration in tqdm(range(50)):\n",
    "        latent.requires_grad_(False)\n",
    "        total_loss = torch.tensor(0, dtype=torch.float)\n",
    "        grads = []\n",
    "        for coeff, loss_fn in losses_fn:\n",
    "            grad_fn, loss = loss_fn(latent)\n",
    "            total_loss += coeff * loss\n",
    "            grads.append(grad_fn)    \n",
    "        total_loss.backward()\n",
    "\n",
    "        total_grad = sum(map(lambda grad_fn: grad_fn(), grads))\n",
    "        optimizer.zero_grad()\n",
    "        latent.requires_grad_(True)\n",
    "        F.softmax(latent, dim=1).backward(gradient = total_grad)\n",
    "        optimizer.step()\n",
    "\n",
    "        grads.append(latent.grad.norm().item())\n",
    "    \n",
    "    print(total_loss.item())\n",
    "    sampled = sample_from_latent(latent, 5, base_vocab, verbose=True)\n",
    "    print(text_from_sample(sampled, base_vocab, length=8))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
